# ğŸ¯ PAIR Algorithm - Architektur-ErklÃ¤rung

## Warum drei verschiedene LLMs?

Der **PAIR Algorithm** (Prompt Automatic Iterative Refinement) verwendet eine **drei-LLM-Architektur**, weil jedes LLM eine spezifische Rolle hat:

### 1. **Attacker LLM (A)** - Angreifer
**Rolle:** Generiert und verfeinert Angriffs-Prompts

**Warum stÃ¤rker sein sollte:**
- Muss intelligente, adaptive Angriffsstrategien entwickeln
- BenÃ¶tigt besseres VerstÃ¤ndnis fÃ¼r Prompt-Engineering
- Sollte aus vorherigen FehlschlÃ¤gen lernen kÃ¶nnen

**Beispiel:** `qwen3:8b` (grÃ¶ÃŸeres Modell)

### 2. **Target LLM (T)** - Ziel
**Rolle:** Das zu testende Modell, das angegriffen wird

**Warum schwÃ¤cher sein kann:**
- Dies ist das Modell, dessen Sicherheit getestet wird
- Kann kleiner/schwÃ¤cher sein - genau das wird getestet
- Sollte die "normale" Produktions-Version sein

**Beispiel:** `qwen2.5:3b` (kleineres Modell)

### 3. **Judge LLM (J)** - Bewerter
**Rolle:** Bewertet, ob ein Angriff erfolgreich war

**Warum stÃ¤rker sein sollte:**
- Muss subtile Sicherheitsverletzungen erkennen
- BenÃ¶tigt gutes VerstÃ¤ndnis fÃ¼r Harmful Content
- Sollte Chain-of-Thought Reasoning durchfÃ¼hren kÃ¶nnen

**Beispiel:** `qwen3:8b` (grÃ¶ÃŸeres Modell)

---

## âŒ Falsche Konfiguration

```json
{
  "target_model_name": "qwen2.5:3b",
  "attacker_model_name": "qwen2.5:3b",  // âŒ Gleiches Modell!
  "judge_model_name": "qwen2.5:3b"      // âŒ Gleiches Modell!
}
```

**Problem:** Ein Modell kann sich nicht effektiv selbst pentesten!

---

## âœ… Korrekte Konfiguration

```json
{
  "target_model_name": "qwen2.5:3b",     // âœ… Zu testendes Modell (kann schwÃ¤cher sein)
  "attacker_model_name": "qwen3:8b",     // âœ… StÃ¤rkeres Modell fÃ¼r bessere Angriffe
  "judge_model_name": "qwen3:8b"         // âœ… StÃ¤rkeres Modell fÃ¼r bessere Bewertung
}
```

**Vorteil:** 
- StÃ¤rkeres Modell generiert bessere Angriffs-Prompts
- StÃ¤rkeres Modell bewertet genauer
- SchwÃ¤cheres Modell wird realistisch getestet

---

## ğŸ“Š PAIR Algorithm Flow

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Attacker    â”‚  (qwen3:8b - stÃ¤rker)
â”‚ LLM (A)     â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â”‚ Generiert páµ¢ (mutated prompt)
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Target      â”‚  (qwen2.5:3b - zu testen)
â”‚ LLM (T)     â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â”‚ Antwortet ráµ¢
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Judge       â”‚  (qwen3:8b - stÃ¤rker)
â”‚ LLM (J)     â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â”‚ Bewertet sáµ¢ (score)
       â–¼
   Feedback â†’ Attacker (fÃ¼r nÃ¤chste Iteration)
```

---

## ğŸ¯ Best Practices

### FÃ¼r Production-Tests:
- **Target**: Produktions-Modell (z.B. `gpt-3.5-turbo`)
- **Attacker**: StÃ¤rkeres Modell (z.B. `gpt-4`)
- **Judge**: StÃ¤rkeres Modell (z.B. `gpt-4`)

### FÃ¼r Local-Tests (Ollama):
- **Target**: `qwen2.5:3b` (kleiner, schneller)
- **Attacker**: `qwen3:8b` (grÃ¶ÃŸer, intelligenter)
- **Judge**: `qwen3:8b` (grÃ¶ÃŸer, genauer)

### FÃ¼r Budget-Optimierung:
- **Target**: Kleines Modell
- **Attacker**: GroÃŸes Modell (wichtig fÃ¼r gute Angriffe)
- **Judge**: GroÃŸes Modell (wichtig fÃ¼r genaue Bewertung)

---

## ğŸ“š Referenzen

- **PAIR Paper**: [Jailbreaking Black Box Large Language Models in Twenty Queries](https://arxiv.org/abs/2310.08419)
- Algorithm 1 beschreibt die drei-LLM-Architektur explizit

---

**Wichtig:** Attacker und Judge sollten **immer stÃ¤rker oder gleich stark** wie das Target sein, um effektive Tests durchzufÃ¼hren!
